import os
import csv
import random
import re
import glob
import os  # æ·»åŠ ç¼ºå°‘çš„osæ¨¡å—å¯¼å…¥
from difflib import SequenceMatcher
import traceback
import requests
from urllib.parse import urlparse
import time


def generate_article_content(client, model_name, outline, title, selected_images=None):
    """Generate high-quality article content"""

    # Build detailed article generation prompt
    prompt = f"""
You are a professional SEO content writing expert. Please generate a high-quality SEO-optimized article based on the following outline and title.

**Article Title**: {title}
**Article Outline**: {outline}

**Writing Requirements**:
1. **Article Structure**:
   - Use clear Markdown formatting
   - # for main title
   - ## for main sections
   - ### for subsections
   - Do NOT include table of contents

2. **Content Quality**:
   - Target word count: 2500-3500 words
   - Content must be original, valuable, and in-depth
   - Avoid repetition and filler content
   - Each paragraph must contain substantial content
   - Provide specific examples and practical advice

3. **SEO Optimization**:
   - Naturally integrate relevant keywords
   - Use semantically related vocabulary
   - Create valuable content links
   - Include practical FAQ section

4. **Format Requirements**:
   - Use **bold** to emphasize key points
   - Use *italic* for definitions
   - Use > for important quotes or callouts
   - Use - or * for lists
   - Use ```code blocks``` for technical content
   - Use tables for comparison data

5. **Article Structure Template**:
**Important Reminders**:
- Ensure each section has substantial content, don't repeat to fill word count
- Content should be logical and coherent
- Provide practical value, not empty theory
- Language should be professional but accessible
- Do NOT include any table of contents or navigation sections

Please begin writing:
"""

    try:
        # å°è¯•ç”Ÿæˆæ–‡ç« å†…å®¹
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=4000,
            presence_penalty=0.1,  # æ·»åŠ presence_penaltyä»¥å‡å°‘é‡å¤
            frequency_penalty=0.1  # æ·»åŠ frequency_penaltyä»¥å¢åŠ ç”¨è¯å¤šæ ·æ€§
        )

        # æ£€æŸ¥å“åº”æ˜¯å¦æœ‰æ•ˆ
        if not response or not response.choices:
            print("Error: Empty response from API")
            return None

        # æå–å¹¶æ¸…ç†æ–‡ç« å†…å®¹
        article_content = response.choices[0].message.content.strip()
        if not article_content:
            print("Error: Empty article content")
            return None

        # æ£€æŸ¥æ–‡ç« å†…å®¹é•¿åº¦
        if len(article_content.split()) < 100:  # å†…å®¹è¿‡çŸ­å¯èƒ½è¡¨ç¤ºç”Ÿæˆä¸å®Œæ•´
            print("Error: Article content too short")
            return None

        return article_content

    except Exception as e:
        print(f"Error generating article: {str(e)}")
        # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
        print(f"Detailed error: {traceback.format_exc()}")
        return None


def fix_format_issues(content):
    """Fix formatting issues"""
    if not content:
        return content

    # Fix heading format
    content = re.sub(r'^#{4,}', '###', content, flags=re.MULTILINE)

    # Ensure headings have blank lines before and after
    content = re.sub(r'\n(#{1,3}\s+[^\n]+)\n', r'\n\n\1\n\n', content)

    # Fix list format
    content = re.sub(r'\n([*-]\s+)', r'\n\n\1', content)

    # Remove excessive blank lines
    content = re.sub(r'\n{3,}', '\n\n', content)

    return content.strip()


def search_local_images(keywords, image_dir="pixabay_ai_images", max_images=3):
    """Search for local images based on keywords"""
    if not os.path.exists(image_dir):
        return []

    # Clean and split keywords
    clean_keywords = []
    for keyword in keywords:
        # Remove special characters, convert to lowercase
        clean_keyword = re.sub(r'[^a-zA-Z0-9\s]', '', keyword.lower())
        clean_keywords.extend(clean_keyword.split())

    # Get all image files
    image_files = glob.glob(f"{image_dir}/*.webp")

    # Calculate match scores
    matches = []
    for image_file in image_files:
        filename = os.path.basename(image_file).lower()
        score = 0

        for keyword in clean_keywords:
            if keyword in filename:
                score += len(keyword) * 2  # Higher score for exact matches
            else:
                # Use similarity matching
                similarity = SequenceMatcher(None, keyword, filename).ratio()
                if similarity > 0.3:
                    score += similarity

        if score > 0:
            matches.append((image_file, score))

    # Sort by score and return top N
    matches.sort(key=lambda x: x[1], reverse=True)
    return [match[0] for match in matches[:max_images]]


def insert_images_from_local(content, keywords, image_dir="pixabay_ai_images", github_username="xiaotiancaixmy", repo_name="seo-article-gen", branch="main"):
    """Insert GitHub-hosted images into article"""
    if not content:
        return content
    
    # æ£€æŸ¥å†…å®¹ä¸­æ˜¯å¦å·²ç»æœ‰å›¾ç‰‡ï¼Œé¿å…é‡å¤æ’å…¥
    if '![' in content and 'github.com' in content:
        print("ğŸ“· å›¾ç‰‡å·²å­˜åœ¨ï¼Œè·³è¿‡é‡å¤æ’å…¥")
        return content

    # Search for relevant images
    selected_images = search_local_images(keywords, image_dir, max_images=3)

    if not selected_images:
        print("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…çš„å›¾ç‰‡æ–‡ä»¶")
        return content

    lines = content.split('\n')
    new_lines = []
    image_index = 0
    inserted_after_title = False

    for i, line in enumerate(lines):
        new_lines.append(line)

        # Insert first image after H1 title (only once)
        if line.startswith('# ') and not inserted_after_title and image_index < len(selected_images):
            image_path = selected_images[image_index]
            image_name = os.path.basename(image_path)
            github_url = f"https://raw.githubusercontent.com/{github_username}/{repo_name}/{branch}/pixabay_ai_images/{image_name}"
            alt_text = f"Illustration about {keywords[0] if keywords else 'topic'}"
            new_lines.append('')
            new_lines.append(f'![{alt_text}]({github_url})')
            new_lines.append('')
            image_index += 1
            inserted_after_title = True
            print(f"ğŸ“· å·²æ’å…¥æ ‡é¢˜åå›¾ç‰‡: {image_name}")

        # Insert images after main H2 headings
        elif line.startswith('## ') and image_index < len(selected_images):
            # Skip table of contents, FAQ, conclusion sections
            if not any(skip_word in line.lower() for skip_word in ['contents', 'table of contents', 'faq', 'conclusion', 'summary']):
                image_path = selected_images[image_index]
                image_name = os.path.basename(image_path)
                github_url = f"https://raw.githubusercontent.com/{github_username}/{repo_name}/{branch}/pixabay_ai_images/{image_name}"
                section_title = line.replace('## ', '').strip()
                alt_text = f"Image related to {section_title}"
                new_lines.append('')
                new_lines.append(f'![{alt_text}]({github_url})')
                new_lines.append('')
                image_index += 1
                print(f"ğŸ“· å·²æ’å…¥ç« èŠ‚å›¾ç‰‡: {image_name} for {section_title}")

    print(f"ğŸ“· æ€»å…±æ’å…¥äº† {image_index} å¼ å›¾ç‰‡")
    return '\n'.join(new_lines)


def count_words(text):
    """Count words in both English and Chinese"""
    if not text:
        return 0

    # Remove Markdown formatting
    clean_text = re.sub(r'[#*`>\-\[\]\(\)]', '', text)
    clean_text = re.sub(r'!\[.*?\]\(.*?\)', '', clean_text)  # Remove images

    # Count Chinese characters
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', clean_text))

    # Count English words
    english_words = len(re.findall(r'\b[a-zA-Z]+\b', clean_text))

    return chinese_chars + english_words


def check_format_and_grammar(content):
    """Check format and basic grammar"""
    issues = []

    if not content:
        return ["Content is empty"]

    # Check for main title
    if not re.search(r'^# ', content, re.MULTILINE):
        issues.append("Missing main title (# )")

    # Check for section headings
    if not re.search(r'^## ', content, re.MULTILINE):
        issues.append("Missing section headings (## )")

    # Check word count
    word_count = count_words(content)
    if word_count < 2000:
        issues.append(f"Insufficient word count: {word_count} < 2000")
    elif word_count > 4000:
        issues.append(f"Excessive word count: {word_count} > 4000")

    return issues


def save_article(content, output_dir, filename):
    """Save article to file"""
    os.makedirs(output_dir, exist_ok=True)
    filepath = os.path.join(output_dir, filename)

    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)

    return filepath


def validate_and_fix_links(content, client, model_name):
    """éªŒè¯å¹¶ä¿®å¤æ–‡ç« ä¸­çš„å¤–éƒ¨é“¾æ¥"""
    import re
    
    # æå–æ‰€æœ‰å¤–éƒ¨é“¾æ¥
    link_pattern = r'\[([^\]]+)\]\(([^\)]+)\)'
    links = re.findall(link_pattern, content)
    
    fixed_content = content
    
    for link_text, url in links:
        # è·³è¿‡å›¾ç‰‡é“¾æ¥å’Œå†…éƒ¨é“¾æ¥
        if url.startswith('http') and not 'github.com' in url and not 'pixabay.com' in url:
            if not is_valid_url(url):
                print(f"âš ï¸ å‘ç°æ— æ•ˆé“¾æ¥: {url}")
                # æ ¹æ®é“¾æ¥æ–‡æœ¬å’Œä¸Šä¸‹æ–‡é‡æ–°æœç´¢æœ‰æ•ˆé“¾æ¥
                new_url = find_alternative_link(link_text, client, model_name)
                if new_url:
                    fixed_content = fixed_content.replace(f'[{link_text}]({url})', f'[{link_text}]({new_url})')
                    print(f"âœ… å·²æ›¿æ¢ä¸º: {new_url}")
                else:
                    # å¦‚æœæ‰¾ä¸åˆ°æ›¿ä»£é“¾æ¥ï¼Œç§»é™¤è¯¥é“¾æ¥ä½†ä¿ç•™æ–‡æœ¬
                    fixed_content = fixed_content.replace(f'[{link_text}]({url})', link_text)
                    print(f"âŒ å·²ç§»é™¤æ— æ•ˆé“¾æ¥ï¼Œä¿ç•™æ–‡æœ¬: {link_text}")
    
    return fixed_content

def is_valid_url(url, timeout=10):
    """æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ"""
    try:
        # åŸºæœ¬URLæ ¼å¼æ£€æŸ¥
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            return False
        
        # å‘é€HEADè¯·æ±‚æ£€æŸ¥å¯è®¿é—®æ€§
        response = requests.head(url, timeout=timeout, allow_redirects=True)
        return response.status_code < 400
    
    except Exception as e:
        print(f"URLéªŒè¯å¤±è´¥ {url}: {e}")
        return False

def find_alternative_link(link_text, client, model_name):
    """æ ¹æ®é“¾æ¥æ–‡æœ¬æ‰¾åˆ°æ›¿ä»£çš„æœ‰æ•ˆé“¾æ¥"""
    try:
        prompt = f"""
è¯·ä¸ºä»¥ä¸‹å†…å®¹æ‰¾åˆ°ä¸€ä¸ªçœŸå®æœ‰æ•ˆçš„ç›¸å…³ç½‘ç«™é“¾æ¥ï¼š

é“¾æ¥æè¿°ï¼š{link_text}

è¦æ±‚ï¼š
1. å¿…é¡»æ˜¯çœŸå®å­˜åœ¨çš„ç½‘ç«™
2. å†…å®¹ä¸æè¿°ç›¸å…³
3. ä¼˜å…ˆé€‰æ‹©æƒå¨ç½‘ç«™ï¼ˆå¦‚å®˜æ–¹ç½‘ç«™ã€çŸ¥ååª’ä½“ã€å­¦æœ¯æœºæ„ç­‰ï¼‰
4. åªè¿”å›URLï¼Œä¸è¦å…¶ä»–å†…å®¹
5. å¦‚æœæ‰¾ä¸åˆ°åˆé€‚çš„é“¾æ¥ï¼Œè¿”å›"NONE"

ç¤ºä¾‹ï¼š
- å¦‚æœæ˜¯"OpenAIå®˜æ–¹ç½‘ç«™"ï¼Œè¿”å›ï¼šhttps://openai.com
- å¦‚æœæ˜¯"arXivè®ºæ–‡"ï¼Œè¿”å›ï¼šhttps://arxiv.org
- å¦‚æœæ˜¯"GitHub"ï¼Œè¿”å›ï¼šhttps://github.com

è¯·è¿”å›URLï¼š
"""
        
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=100
        )
        
        suggested_url = response.choices[0].message.content.strip()
        
        # éªŒè¯å»ºè®®çš„URL
        if suggested_url != "NONE" and is_valid_url(suggested_url):
            return suggested_url
        
    except Exception as e:
        print(f"æŸ¥æ‰¾æ›¿ä»£é“¾æ¥å¤±è´¥: {e}")
    
    return None


def check_format_and_grammar(content):
    """Check format and basic grammar"""
    issues = []

    if not content:
        return ["Content is empty"]

    # Check for main title
    if not re.search(r'^# ', content, re.MULTILINE):
        issues.append("Missing main title (# )")

    # Check for section headings
    if not re.search(r'^## ', content, re.MULTILINE):
        issues.append("Missing section headings (## )")

    # Check word count
    word_count = count_words(content)
    if word_count < 2000:
        issues.append(f"Insufficient word count: {word_count} < 2000")
    elif word_count > 4000:
        issues.append(f"Excessive word count: {word_count} > 4000")

    return issues


def save_article(content, output_dir, filename):
    """Save article to file"""
    os.makedirs(output_dir, exist_ok=True)
    filepath = os.path.join(output_dir, filename)

    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)

    return filepath


def generate_article_from_outline(client, model_name, outline, title, keywords=None, output_dir="output/articles"):
    """Generate complete article from outline"""
    print(f"Starting article generation: {title}")
    print(f"ğŸ“ å…³é”®è¯: {keywords}")

    # Generate article content
    content = generate_article_content(client, model_name, outline, title)

    if not content:
        print("âŒ Article generation failed")
        return None

    # Fix formatting issues
    content = fix_format_issues(content)

    # Insert relevant images (ç¡®ä¿æœ‰å…³é”®è¯æ—¶æ‰æ’å…¥)
    if keywords and len(keywords) > 0:
        print("ğŸ“· å¼€å§‹æ’å…¥å›¾ç‰‡...")
        content = insert_images_from_local(content, keywords)
    else:
        print("âš ï¸ æ²¡æœ‰å…³é”®è¯ï¼Œè·³è¿‡å›¾ç‰‡æ’å…¥")
    
    # éªŒè¯å’Œä¿®å¤å¤–éƒ¨é“¾æ¥
    print("ğŸ” éªŒè¯æ–‡ç« ä¸­çš„å¤–éƒ¨é“¾æ¥...")
    content = validate_and_fix_links(content, client, model_name)

    # Check content quality
    issues = check_format_and_grammar(content)

    if issues:
        print(f"âš ï¸ Issues found: {', '.join(issues)}")

        # If word count is insufficient, try to optimize rather than simply rewrite
        if any("Insufficient word count" in issue for issue in issues):
            print("Optimizing content length...")

            optimize_prompt = f"""
Please optimize the following article to make it more detailed and complete, but do not repeat existing content.

Current article:
{content}

Optimization requirements:
1. Add more practical details and examples to existing sections
2. Expand each point with deeper explanations
3. Add more real-world application scenarios
4. Ensure content is valuable, don't repeat just to fill word count
5. Maintain original structure and logic
6. Target word count: 2500-3500 words
7. make sure H3 details need to have at least 100 words for each.
8. DO NOT remove or modify existing images

Please return the optimized complete article:
"""

            try:
                response = client.chat.completions.create(
                    model=model_name,
                    messages=[{"role": "user", "content": optimize_prompt}],
                    temperature=0.6,
                    max_tokens=4000
                )
                optimized_content = response.choices[0].message.content.strip()

                if optimized_content:
                    content = fix_format_issues(optimized_content)
                    # ä¸è¦é‡å¤æ’å…¥å›¾ç‰‡ï¼Œå› ä¸ºä¼˜åŒ–åçš„å†…å®¹åº”è¯¥ä¿ç•™åŸæœ‰å›¾ç‰‡
                    print("âœ… Content optimization completed")

            except Exception as e:
                print(f"Error during optimization: {e}")

    # Final check
    final_word_count = count_words(content)
    print(f"ğŸ“Š Final word count: {final_word_count}")

    # Save article
    timestamp = random.randint(1000, 9999)
    filename = f"article_{timestamp}.md"
    filepath = save_article(content, output_dir, filename)

    print(f"âœ… Article saved: {filepath}")
    return filepath


def generate_articles_from_outlines(client, model_name, input_file, output_dir, metadata_file=None):
    """Batch generate articles from outline file"""
    with open(input_file, 'r', newline='', encoding='utf-8') as f:
        reader = csv.reader(f)
        next(reader)  # Skip header row
        outlines = [row[0] for row in reader]

    # Try to read keywords (if available)
    keywords_list = []
    keyword_file = "data/keyword.csv"
    if os.path.exists(keyword_file):
        with open(keyword_file, 'r', newline='', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader)  # Skip header row
            keywords_list = [row[0].split(',') if row else [] for row in reader]

    # Try to read titles (if available)
    titles_list = []
    title_file = "data/title.csv"
    if os.path.exists(title_file):
        with open(title_file, 'r', newline='', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader)  # Skip header row
            titles_list = [row[0] if row else f"Article {i+1}" for i, row in enumerate(reader)]

    generated_files = []

    for i, outline in enumerate(outlines):
        title = titles_list[i] if i < len(titles_list) else f"Article {i+1}"
        keywords = keywords_list[i] if i < len(keywords_list) else []

        filepath = generate_article_from_outline(
            client, model_name, outline, title, keywords, output_dir
        )

        if filepath:
            generated_files.append(filepath)

        print(f"Progress: {i+1}/{len(outlines)}")
        print("-" * 50)

    print(f"\nğŸ‰ Batch generation completed! Generated {len(generated_files)} articles")
    print(f"Articles saved in: {output_dir}")

    return generated_files